Since I used accuracy as the main metric to measure performance, "better/lower performance" henceforth will be analogous to "higher/lower accuracy on test set". 

First, I tried adjusting the number of convolutional, hidden, and pooling layers. I realized increasing the number of layers did not necessarily have a positive impact on performance; infact, increasing the number of pooling and hidden layers lowered performance. I also tried adjusting the number of neurons, or units, within each layer. This too did not show much improvement apart from the convolutional layer. However, the advantages seemed to cap off after a certain number, and blindly continuing to increase the number of convolutional layers would probably only lengthen runtime sans any benefits. Lowering drop out rates for hidden layers (0.5 -> 0.2) showed clear improvement, but I am guessing this is due to overfitting. Lastly, I tried adjusting the dimensions of the kernel for convolutional layers as well as pooling sizes for pooling layers. While increasing pooling sizes didn't have much effect, using a 5 by 5 kernel instead of a 2 by 2 or 3 by 3 showed mild improvement. 